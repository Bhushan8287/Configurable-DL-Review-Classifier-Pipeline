
# ğŸ“¦ ConfigurableDLReviewClassifierPipeline

An end-to-end, **modular and configurable deep learning pipeline** for sentiment classification of movie reviews using RNN-based models â€” BiLSTM, LSTM, and GRU.

This project automates the full ML lifecycle: from data preprocessing, splitting, training, evaluation, saving outputs, and serving predictions manually via a Streamlit interface.

> âš ï¸ **Note:** Streamlit integration is manual. You must manually select a model and update paths in `app.py`.

---

## ğŸ¯ Key Features

* ğŸ”§ **Fully configurable** via `config.yaml`
* ğŸ“ **Modular folder structure** for clarity and reusability
* ğŸ§  Supports 3 RNN-based models: `Bi-LSTM`, `LSTM`, `GRU`
* â™»ï¸ Easily extendable to other DL/NLP models
* ğŸ“Š Automatic saving of metrics, plots, models, tokenizer, thresholds
* ğŸ“ **Logging** for all components to trace pipeline steps
* ğŸŒ **Streamlit** app for serving predictions (manual setup)

---

## ğŸ› ï¸ Tech Stack

* **Language**: Python 3.10
* **Libraries**:

  * `TensorFlow 2.19.0`
  * `Scikit-learn 1.6.1`
  * `Matplotlib 3.10`
  * `Streamlit 1.45.1`
  * `PyYAML 6.0.2`
  * `Joblib 1.5.1`

---

## ğŸ“‚ Project Structure

```
ConfigurableDLReviewClassifierPipeline/
â”‚
â”œâ”€â”€ dataset/                      # Raw and cleaned dataset
â”‚   â””â”€â”€ movie.csv, cleaned_dataset.csv
â”‚
â”œâ”€â”€ logs/                         # Logging
â”‚   â””â”€â”€ pipeline_logs.log
â”‚
â”œâ”€â”€ notebook/                     # Jupyter experiments and artifacts
â”‚   â””â”€â”€ experiments.ipynb, prediction.ipynb, tokenizer.pkl, maxlength.pkl, best_model.keras
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ metrics/                  # JSON evaluation metrics for all models
â”‚   â”œâ”€â”€ plots/                    # Accuracy/loss + ROC plots
â”‚   â”œâ”€â”€ preprocessing_artifacts/ # Tokenizer + max length
â”‚   â””â”€â”€ trained_models/          # Saved .keras models + thresholds
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/                  # Config loader
â”‚   â”œâ”€â”€ data/                    # Data loading
â”‚   â”œâ”€â”€ model_training/         # Training, evaluation, thresholding
â”‚   â”‚   â””â”€â”€ model_definitions/  # GRU, LSTM, BiLSTM
â”‚   â”œâ”€â”€ pipeline/               # Main full pipeline logic
â”‚   â”œâ”€â”€ pre_processing/         # Data cleaning, splitting, tokenizing
â”‚   â””â”€â”€ utils/                  # Logging, helper functions
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ app.py                      # Streamlit app
â”œâ”€â”€ config.yaml                 # Config file for the entire pipeline
â”œâ”€â”€ main.py                     # Entry point for the pipeline
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ How It Works

The pipeline uses a single config file to control key behavior and parameters. Here's a high-level flow:

1. **Loads the dataset**
2. **Preprocesses the data** (cleaning, splitting, tokenizing)
3. **Trains the selected model** (BiLSTM/LSTM/GRU)
4. **Evaluates performance** using default threshold
5. **Finds optimal threshold** using ROC analysis
6. **Generates and saves** metrics, plots, models, tokenizer, and thresholds

---

## ğŸ” Example Config (`config.yaml`)

```yaml
dataset:
  path: "dataset/movie.csv"
  cleaned_dataset_save_path: "dataset/cleaned_dataset.csv"
  text_column: "text"
  label_column: "label"

dataset_splitting:
  test_size: 0.2
  random_state: 42
  split_save_path: "dataset"

tokenization:
  num_of_words: 10000
  max_length: 200
  oov_token: "<OOV>"
  padding: "post"
  truncating: "post"
  save_tokenizer_path: "outputs/preprocessing_artifacts/tokenizer.joblib"
  save_max_length: "outputs/preprocessing_artifacts/max_length.json"
```

---

## ğŸš€ Getting Started

### 1. Clone the Repo

```bash
git clone https://github.com/your-username/ConfigurableDLReviewClassifierPipeline.git
cd ConfigurableDLReviewClassifierPipeline
```

### 2. Install Requirements

```bash
pip install -r requirements.txt
```

### 3. Update Config

Update `config.yaml` paths and parameters to match your environment.
Then point `config_loader.py` to your `config.yaml` path.

### 4. Run the Pipeline

```bash
python main.py
```

### 5. View Outputs

* ğŸ“ `outputs/` â†’ Metrics, plots, models
* ğŸ“ `logs/pipeline_logs.log` â†’ Full logs for debugging

---

## ğŸŒ Streamlit App (Manual Setup)

1. Choose model & tokenizer from `outputs/trained_models` and `outputs/preprocessing_artifacts`
2. Update the corresponding file paths manually in `app.py`
3. Run:

```bash
streamlit run app.py
```

Then open browser at: `http://localhost:8501`

---

## ğŸ“Š Evaluation Metrics

* Accuracy
* Precision
* Recall
* F1 Score
* Confusion Matrix

---

## ğŸ“Œ Notes

* Designed for demonstrating **clean code**, **config-driven architecture**, and **deep learning text classification**
* Streamlit integration is **manual** by design, for flexibility

---

## âœ… What I Learned

* Structuring modular deep learning pipelines for NLP
* Designing reusable, clean architecture for real-world projects
* Logging, output traceability, and reproducibility
* Trade-offs between manual vs. automated deployment

---

Let me know if you'd like a matching `LICENSE`, project badge suggestions, or `README` badges added!
